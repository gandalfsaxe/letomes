# Software engineering

The project has been defined by a relatively wide scope of disciplines that we have worked on. Mathematic derivations, astrophysics intuition, machine learning, high-performance computing, and software engineering. The last of those particularly because of the simultaneous need for rapid prototyping and strong performance characteristics. Essentially, we needed to be able to test a plethora of different ideas to answer all our questions, but since the computations are so relatively expensive, we wanted to build a few solid modules that could be treated as black boxes for our overarching purposes. Essentially, we rewrote the simulator from the precursor project from the ground up, with a focus on modularity and extensibility. This was done on basis of the model inherent in the original, to give us a consistent 'ground truth' with which to compare our new simulator.

We packaged the new simulator into its own python module, installable with pip, to ensure proper reproducibility across machines and different python configurations. We then implemented the simulator into our ES framework, which we had previously developed using mock data, and tuned its hyperparameters to let it function as a decent search strategy for the lunar case. As we pursued this goal, performance became a severe issue. The ES algorithm wants to compute trajectories thousands or millions of times, and that was not feasible with each trajectory taking between 60 and 300 seconds to compute, so we delved into the simulator again in order to optimize it with regards to runtime. 

## Numba

Python is a notoriously slow language, due to its paradigm of not compiling code, but simply running on the script itself; with Numba however, we can mitigate this problem, without sacrificing too much of the very high productivity that Python offers. We attempted to have Numba optimize our code for us, but found that our "proper software engineering" philosophy was preventing this from working to any meaningful effect. Numba is only able to give significant performance increases if working with basic types: Integers, Floats, Strings, and lists of those, more or less. As soon as complicated data structures are introduced into the mix, Numba drops to 'object mode', where it attempts to find subsections of the function that are purely defined with basic types. This mode was useless for us simply making the code run slower due to the overhead of the compilation.

In order to get the performance increases that the problem required, we had to make some sacrifices in our idealized architecture, ditching any complicated return types, any use of exceptions and the 'Planet' class that we were using to contain our data. This was a painful sacrifice, since it enabled us to simply call the algorithm with a named celestial body, and instantly have all the relevant parameters in place for computing a trajectory. Instead, we have been forced to split it into several functions with duplicated code, and a significant increase in visual and lexicographical complexity; a large reason for the redesign happening in the first place. Regrettable, but a valuable learning experience.

All this heartwrenching work was not for nothing however, since with the precompilation of the symplectic St√∂rmer-Verlet algorithm in place, we were seeing speedups of a factor 2-300. 

Once we had this architecture in place, and giving the same results as the original simulator, we built extensions to it to cover the martian travel case unique to this project. <TODO more on this when mars is working>

## CUDA

We attempted to gain additional performance from our simulator, we tried to run it on GPUs. Given that the fitness evaluations are completely independent in ES, it is a great candidate for massive parallelization. This is also doable with Numba, but requires an even more granular refactoring of the simulator code, and conscious management of data transfer between CPU and GPU, since the communication overhead associated with GPU work can very easily eclipse the performance gained by the massive parallelization. We attempted to make the necessary changes, but in the end we could not get it to work, and decided that the medium level of parallelization that we could achieve easily would have to be enough. The HPC CPU clusters have two 2.8GHz 10-core Intel Xeon processors, so they are nothing to scoff at by themselves. Still, performance has been a limiting factor for the types and number of experiments we could do on the ML front, so in the future, we would like to pursue the GPU angle further.

## Docker

In our attempts to do things "The Right Way" from the start, we pursued docker for some time near the start of the project. The appeal of docker is that it gives a program total platform independence, since one delivers the program bundled together with a 'container'; a miniature virtual machine, that has all the necessary software in exactly the correct versions with the same configuration as the environment in which the program is run by the developer. We created such a container, and were conducting our initial experiments in it, but regrettably, the HPC cluster that was available to us could not allow the installation of docker, since parts of it run with root access. Since the HPC cluster is shared by the entire institution, this is not permitted by procedure, so the concept was abandoned, and we took the time to manually initialize the environments that the program was to run on.
