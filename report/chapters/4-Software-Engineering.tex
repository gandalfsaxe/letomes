% !TEX root =../thesis-letomes.tex

\chapter{Software Engineering}

The project has been defined by a relatively wide scope of disciplines that we have worked on. Mathematic derivations, astrophysics intuition, machine learning, high-performance computing, and software engineering. The last of those particularly because of the simultaneous need for rapid prototyping and strong performance characteristics. Essentially, we needed to be able to test a plethora of different ideas to answer all our questions, but since the computations are so relatively expensive, we wanted to build a few solid modules that could be treated as black boxes for our overarching purposes. Essentially, we rewrote the simulator from the precursor project from the ground up, with a focus on modularity and extensibility. This was done on basis of the model inherent in the original, to give us a consistent  with which to compare our new simulator.

We packaged the new simulator into its own python module, installable with pip, to ensure proper reproducibility across machines and different python configurations. We then implemented the simulator into our ES framework, which we had previously developed using mock data, and tuned its hyperparameters to let it function as a decent search strategy for the lunar case. As we pursued this goal, performance became a severe issue. The ES algorithm wants to compute trajectories thousands or millions of times, and that was not feasible with each trajectory taking \SIrange{60}{300}{\second} to compute, so we delved into the simulator again in order to optimize it with regards to runtime.

\section{Software Tools and Libraries}

\subsection{Numba}

Python is a notoriously slow language, due to its paradigm of not compiling code, but simply running on the script itself; with Numba however, we can mitigate this problem, without sacrificing too much of the very high productivity that Python offers. We attempted to have Numba optimize our code for us, but found that our "proper software engineering" philosophy was preventing this from working to any meaningful effect. Numba is only able to give significant performance increases if working with basic types: Integers, Floats, Strings, and lists of those, more or less. As soon as complicated data structures are introduced into the mix, Numba drops to `object mode', where it attempts to find subsections of the function that are purely defined with basic types. This mode was useless for us simply making the code run slower due to the overhead of the compilation.

In order to get the performance increases that the problem required, we had to make some sacrifices in our idealized architecture, ditching any complicated return types, any use of exceptions and the \texttt{Planet} class that we were using to contain our data. This was a painful sacrifice, since it enabled us to simply call the algorithm with a named celestial body, and instantly have all the relevant parameters in place for computing a trajectory. Instead, we have been forced to split it into several functions with duplicated code, and a significant increase in visual and lexicographical complexity; a large reason for the redesign happening in the first place. Regrettable, but a valuable learning experience.

All this heart-wrenching work was not for nothing however, since with the pre-compilation of the symplectic Störmer-Verlet algorithm in place, we were seeing speedups of a factor 2-300. 

Once we had this architecture in place, and giving the same results as the original simulator, we built extensions to it to cover the martian travel case unique to this project.\todo{More about software architecture / numba optimization when mars is working}

\subsection{CUDA}

We attempted to gain additional performance from our simulator, we tried to run it on GPUs. Given that the fitness evaluations are completely independent in ES, it is a great candidate for massive parallelization. This is also doable with Numba, but requires an even more granular refactoring of the simulator code, and conscious management of data transfer between CPU and GPU, since the communication overhead associated with GPU work can very easily eclipse the performance gained by the massive parallelization. We attempted to make the necessary changes, but in the end we could not get it to work, and decided that the medium level of parallelization that we could achieve easily would have to be enough. The HPC CPU clusters have two \SI{2.8}{\GHz} 10-core Intel Xeon processors, so they are nothing to scoff at by themselves. Still, performance has been a limiting factor for the types and number of experiments we could do on the ML front, so in the future, we would like to pursue the GPU angle further.

\subsection{Docker}

In our attempts to do things ``The Right Way'' from the start, we pursued docker for some time near the start of the project. The appeal of docker is that it gives a program total platform independence, since one delivers the program bundled together with a `container'; a miniature virtual machine, that has all the necessary software in exactly the correct versions with the same configuration as the environment in which the program is run by the developer. We created such a container, and were conducting our initial experiments in it, but regrettably, the HPC cluster that was available to us could not allow the installation of docker, since parts of it run with root access. Since the HPC cluster is shared by the entire institution, this is not permitted by procedure, so the concept was abandoned, and we took the time to manually initialize the environments that the program was to run on.


\section{Our Orbsim Module}

We have arranged our code into a python module that offers us some nice modularity and extensibility, at least as far as our relatively specific problem is concerned. What follows is an explanation of the architecture of this module.

\subsection{Abstractions}

The simulator is a numerical solver for an analytical problem, implementing algorithms that have been defined through that analysis. Thus, we have taken care to keep nomenclature consistent between the simulator and its analytical foundations. Between code and paper, if you will. This is complicated somewhat by the fact that we are applying Evolution Strategies as our search strategy, which carries with it its own set of nomenclature from the machine learning world. We have kept the two things largely separate, with the ES module interacting with the simulator through a relatively simple interface. This works fine, since ES is a black-box optimizer anyway.

\subsection{Simulator module}

Here, we have the interface between ES portion and simulator.

\noindent The function \texttt{launch\_sim} takes hyperparameters that the ES algorithm uses for its optimization (in the form of a decision vector \(\psi\)), reformulates them, and starts a simulation based on them. It then returns a delta-v for that single run, along with the associated saved path. That delta-v is, as mentioned before, our fitness function, so based on that result, ES can continue to do its job.

\subsection{Integrators module}

This contains the main loop of our simulator algorithm, along with subfunctions for the individual steps.

\subsection{Analyticals module}

This module contains a ton of different convenience functions that compute some intermediate equation for use in the main algorithm. They are hidden away here to reduce clutter in \texttt{Integrators.py}.

\subsection{Planets module}

Here, we define a planet class, which we use to store the various constants associated with a given planet. Note that we also consider the moon a planet for this purpose. We keep information such as mass and safe orbital radius here, again to simplify and improve the readability of the main algorithm. The full list of planetary constants can be seen in \todoref{Planetary constants}

\section{PyKEP - Python Library for Astrodynamics}

\subsection{pykep and PyGMO}

pykep is an open-source engine for formulating and solving problems of space flight nature. It's developed by the European Space Agency, in cooperation with NASA's JPL and the space flight community at large. The library interfaces with data on ephemerides\footnote{object trajectories computed to give position at a given time} for every recorded celestial body and major/minor asteroid. It uses PyGMO as its back end, which is a C++ library\footnote{PaGMO: Parallel Global Multi-objective Optimizer. It is developed by the same people at ESA as pykep} for solving optimization problems, wrapped in python. pykep essentially lets users define their optimization problem in a pedagogical way, redefines it in a format that PyGMO can receive, and lets \emph{it} do all the heavy lifting. pykep then relays that information back to the user.

PyGMO implements a wide variety of algorithms, among which we find both the CMA-ES\footnote{CMA-ES: Covariance Matrix Adaptation Evolution Strategy} and xNES\footnote{xNES: Exponential Natural Evolution Strategy} methods, which are both very relevant for our problem.

\subsection{Implementation Details} 

PyGMO is all about performance through parallelization, and implements a neat metaphor. It creates an `archipelago' of `islands', with populations of `individuals'. These individuals are defined by a `chromosome' (decision vector), which is perturbed each cycle according to whichever algorithm is being run. The best performing individual after some number of cycles is crowned the `champion', and `migrates' to neighboring islands in the archipelago, where they then join their population and fight once more. This continues until the entire archipelago agrees on a champion (convergence). Since the islands are independent while they are fighting, the model makes any algorithm very parallelizable.

\subsection{MGA-1DSM \todo{DELETE}}
We use a pykep mission type called Multiple Gravity Assist with One Deep Space Maneuver (mga*1dsm), which allows one deep space maneuver per leg of the journey: [earth, venus, earth] would define a round trip with two legs: earth-venus and venus-earth, each leg allowing one maneuver. The mission defines a decision vector of the form:
\begin{equation}
    [t0, T]+[u,v,V*{inf},eta_1, \alpha_1] + [\beta_2, \frac{rP}{rV},eta_2,\alpha_2]+ ... +[\beta_n,\frac{rP_n}{rV_n},eta_n,\alpha_n] ,
\end{equation}
where \todo{Explanation of decision vector}

\subsection{PaGMO Implementation Details}

PaGMO provides its functionality through a very sensibly architectured interface. One defines a \emph{Problem}, an \emph{Algorithm}, and an \emph{Archipelago} (or just a single \emph{Island} if no parallelization is desired). The problem gives bounds and a fitness evaluation function, the algorithm exposes an \texttt{evolve} function, which, when called, should execute a single run of the algorithm, whatever its criterion for termination is. The archipelago is parameterized by a problem and an algorithm, as well as how many islands it should contain, and what the population of each island should be. We then call upon the archipelago to begin evolution, which happens `under the hood' as it were. Each individual in an island's population represents a randomly chosen decision vector, or a set of hyperparameters \(\psi\), and the algorithm is run on this annealed set until termination. The result(s) with the highest fitness values (the champions) are then moved to neighboring islands, where they fight other champions from the last generation, until finally the archipelago converges, and each island's champion is returned. Each island runs in a separate thread, with communication only of the very lightweight champion decision vectors between neighbors. As such, the framework is very performant as a way to scale up an algorithm such as ours, especially as it takes care of the annealing that a problem prone to local minima--such as ours--needs.

\subsection{The ES module}

The ES module conducts black-box optimization on a decision vector that describes the initial parameters for a launch. Every time the algorithm wants to find the fitness of a given point in our parameter space, it calls the orbsim module with those parameters, using the returned value as the function to minimize. The algorithm is parallelized through the use of PaGMO, which runs the ES algorithm on multiple cores independently, with many separate starting conditions.

\subsubsection{DERP}

The module defines a PyGMO \emph{problem} class, which gives the bounds and fitness function for our optimization space. The fitness function takes the decision vector \(\psi\) as argument, calls on orbsim to run a simulation with the contained parameters, and returns the result of that simulation: The \(\Delta v\) value, or if the rocket did not hit the target, how close it came at the minimum. The fitness is a single scalar, so the fact that that scalar can mean two different things (a change in velocity or a distance) gives some problems for estimating gradients in our space. We mitigate this by penalizing an unsuccessful mission heavily, by adding an upward bias and weight, as well as squaring the result, to amplify the effect of small differences in minimum distance.

We then define the ES algorithm within the PyGMO framework, which defines an \emph{evolve} function, which executes a single run of the algorithm. This function takes a \emph{population} \(\Psi\), a collection of decision vectors, and for each vector at step \emph{t} \((\psi_{i,t})\) creates several jittered copies of those vectors, akin to creating a gaussian point cloud in the 3-dimensional problem space that the vectors span. Each of these points is then tested (the fitness function is run) and the average of those jittered points, weighted according to fitness score, is deemed as the location for the next point \(\psi_{i,t+1}\). The algorithm then runs until it has converged upon a local minimum, or until some maximum number of steps has been reached.

Next, we create an \emph{archipelago}, another PyGMO concept. It is, as the name suggests, a collection of \emph{islands} which each run the algorithm on their own separate populations. Each island runs in parallel. This convenient metaphor and easy implementation is the main draw of PyGMO for us. 

\section{High Performance Computing}
as it relates to ES with an expensive objective function.

\subsubsection{PaGMO}
making the case for parallelism, given that trajectories are independent, and explaining the metaphor behind PaGMO, and why we think it useful.

\section{GPU programming with CUDA}
massive parallelism vs. data transfer overhead.

\section{Software Engineering}
incl. working with old code, mutability/performance trade-oﬀ.

\subsubsection{Unit Testing}
ensuring correctness of complex system with many edits taking place.