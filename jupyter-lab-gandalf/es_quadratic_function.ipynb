{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Evolution Strategies (NES) toy example that optimizes a quadratic function\n",
    "\n",
    "A bare bones example of optimizing a black-box function (f) using\n",
    "Natural Evolution Strategies (NES), where the parameter distribution is a \n",
    "gaussian of fixed standard deviation.\n",
    "\n",
    "Adapted from: https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d\n",
    "Originally linked from https://blog.openai.com/evolution-strategies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we want to optimize\n",
    "def f(w):\n",
    "  # here we would normally:\n",
    "  # ... 1) create a neural network with weights w\n",
    "  # ... 2) run the neural network on the environment for some time\n",
    "  # ... 3) sum up and return the total reward\n",
    "\n",
    "  # but for the purposes of an example, lets try to minimize\n",
    "  # the L2 distance to a specific solution vector. So the highest reward\n",
    "  # we can achieve is 0, when the vector w is exactly equal to solution\n",
    "  reward = -np.sum(np.square(solution - w))\n",
    "  return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "npop = 50 # population size\n",
    "sigma = 0.1 # noise standard deviation\n",
    "alpha = 0.001 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 w: [-0.69843176  1.28072549  0.69720699], solution: [ 0.5  0.1 -0.3], reward: -3.824773158987239\n",
      "iter 20 w: [-0.58066891  1.15975134  0.5881584 ], solution: [ 0.5  0.1 -0.3], reward: -3.079743534636482\n",
      "iter 40 w: [-0.46523764  1.04138043  0.48773192], solution: [ 0.5  0.1 -0.3], reward: -2.438402393417708\n",
      "iter 60 w: [-0.35270921  0.91490314  0.39706625], solution: [ 0.5  0.1 -0.3], reward: -1.87708147977002\n",
      "iter 80 w: [-0.22040744  0.79679601  0.3051493 ], solution: [ 0.5  0.1 -0.3], reward: -1.3707172273391777\n",
      "iter 100 w: [-0.10843605  0.66833215  0.20260919], solution: [ 0.5  0.1 -0.3], reward: -0.9458118655214777\n",
      "iter 120 w: [0.01204189 0.56321217 0.08864005], solution: [ 0.5  0.1 -0.3], reward: -0.6037097110784341\n",
      "iter 140 w: [ 0.13462939  0.44321302 -0.00830926], solution: [ 0.5  0.1 -0.3], reward: -0.33637435307150726\n",
      "iter 160 w: [ 0.2602041   0.32826352 -0.09911975], solution: [ 0.5  0.1 -0.3], reward: -0.14995918222059737\n",
      "iter 180 w: [ 0.36213197  0.22241824 -0.18699382], solution: [ 0.5  0.1 -0.3], reward: -0.046764214883775176\n",
      "iter 200 w: [ 0.45873     0.14240693 -0.26338733], solution: [ 0.5  0.1 -0.3], reward: -0.004842047908163923\n",
      "iter 220 w: [ 0.49181629  0.10379803 -0.29021515], solution: [ 0.5  0.1 -0.3], reward: -0.000177141500896503\n",
      "iter 240 w: [ 0.49805055  0.10671872 -0.29742954], solution: [ 0.5  0.1 -0.3], reward: -5.554880195097623e-05\n",
      "iter 260 w: [ 0.49591701  0.10211219 -0.28910243], solution: [ 0.5  0.1 -0.3], reward: -0.00013988924479764258\n",
      "iter 280 w: [ 0.5055422   0.10362792 -0.30321174], solution: [ 0.5  0.1 -0.3], reward: -5.419298929618031e-05\n"
     ]
    }
   ],
   "source": [
    "# start the optimization\n",
    "\n",
    "solution = np.array([0.5, 0.1, -0.3])\n",
    "w = np.random.randn(3) # our inital guess is random\n",
    "\n",
    "for i in range(300):\n",
    "    \n",
    "    # print current fitness of the most likely parameter setting\n",
    "    if i % 20 == 0:\n",
    "        print(\"iter {} w: {}, solution: {}, reward: {}\"\n",
    "              .format(i, str(w), str(solution), f(w)))\n",
    "        \n",
    "    # initialize memory for a population of w's, and their rewards\n",
    "    N = np.random.randn(npop,3)\n",
    "    R = np.zeros(npop)\n",
    "    for j in range(npop):\n",
    "        w_try = w + sigma*N[j] # jitter w using guassian of sigma 0.1\n",
    "        R[j] = f(w_try) # evalutate the jittered version\n",
    "    \n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    A = (R - np.mean(R)) / np.std(R)\n",
    "    # perform parameter update. The matrix multiply below\n",
    "    # is just an efficient way to sum up all the rows of the noise matrix N,\n",
    "    # where each row N[j] is weighted by A[j]\n",
    "    w = w + alpha/(npop*sigma) * np.dot(N.T, A)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77651471,  0.36221492, -0.71565331])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
